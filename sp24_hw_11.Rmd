---
title: "Homework 11"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(glmnet)
library(MASS)
```

## Problem #1. Guided k-fold CV exercise <small>9pts</small>

In this exercise, we will guide you through an exercise where you are asked to use k-fold cross validation to evaluate the performance of several models.

For this exercise we will use the "Swiss Fertility and Socioeconomic Indicators (1888)" dataset from the `datasets` package, which is loaded below. (To view the help page, run `?datasets::swiss` in your console). We will be using `Fertility` as our response variable.

```{r}
swiss = datasets::swiss
```


### Part a) Understanding/visualizing data

Read the help page and briefly "introduce" this dataset. Specifically, explain where the data comes from, what variables it contains, and why should people care about the dataset.

Produce one or some visualizations of the data. Do your best here to try to use your plots to help your viewer best understand the structure and patterns of this dataset. Choose your plots carefully and briefly explain what each plot tells you about the data.

> This data provides standardized fertility measures and socio-economic indicators for every French-speaking province of Switzerland (47) in around 1888, and it comes from the Office of Population Research at Princeton University (1976). It contains 6 variables: Fertility, Agriculture, Examination, Education, Catholic, and Infant Mortality, with all but the first giving proportions of the population. People should care about this dataset because it quantitatively shows the demographic transition Switzerland was going through at the time.

> Plot 1 shows the relationship between the % of males with education beyond primary school and the overall fertility rate. It is hard to observe a trend due to the prominence of clustering, but generally, it appears that the less educated a province, the more fertile men it has.
> Plot 2 shows the relationship between the % of males involved in agriculture as an occupation and the overall fertility rate. There is a very weak positive correlation in that in provinces where there is a high % of males involved in agriculture, male fertility is higher.
> Plot 3 shows the relationship between the % of males who are Catholic and the overall fertility rate. There is a lot of clustering occurring at both ends of the plot, with provinces having either very low and very high %s of Catholic men both being highly fertile.

```{r}
ggplot(swiss) +
  geom_point(aes(x = Education, y = Fertility), color = "red") +
  ggtitle("% Education Beyond Primary School v. Fertility Rate")

ggplot(swiss) +
  geom_point(aes(x = Agriculture, y = Fertility), color = "darkgreen") + 
  ggtitle("% Agriculture as an Occupation v. Fertility Rate")

ggplot(swiss) +
  geom_point(aes(x = Catholic, y = Fertility), color = "blue") +
  ggtitle("% Catholic v. Fertility Rate")
```


### Part b) Starting with basic lm

Compare a model with all predictors with no interactions with 2 other models of YOUR choice. Fit all 3 models, show their summary outputs, and briefly comment on which one you think might perform the best when used for future predictions and why.

```{r}
swiss_v1 = lm(Fertility ~ 1 + Education + Examination + Agriculture + Catholic + Infant.Mortality, data = swiss)
summary(swiss_v1)

swiss_v2 = lm(Fertility ~ 1 + Education + Catholic + Education:Catholic, data = swiss)
summary(swiss_v2)

swiss_v3 = lm(Fertility ~ 1 + Catholic + Agriculture + Examination, data = swiss)
summary(swiss_v3)
```

> I think the first model might perform the best when used for future predictions because it has both the highest R-squared value and lowest residual standard error.

### Part c) Estimating MSE using CV

Now, we are going to actually estimate the MSE of each model with K-fold cross validation. First we're going to set a seed and import the `caret` package (it should be already installed since it's a prerequisite for many other packages, but if it's not for some reason, you can install it with `install.packages("caret")`)

```{r}
set.seed(1)
library(caret)
```

Next, use the following chunk, which already has `method` set to `lm`, `data` set to the `swiss` data set, and validation method set to use 5-fold CV, to estimate the MSE of each of your models. All you need to do is add in a formula for your model and repeat for all 3 models you have.

```{r,error=T}
set.seed(1)

model_1 = train(Fertility ~ 1 + Education + Examination + Agriculture + 
    Catholic + Infant.Mortality, method = "lm", data = swiss, trControl = trainControl(method = "cv", number = 5))
model_2 = train(Fertility ~ 1 + Education + Catholic + Education:Catholic, method = "lm", data = swiss, trControl = trainControl(method = "cv", number = 5))
model_3 = train(Fertility ~ 1 + Catholic + Agriculture + Examination, method = "lm", data = swiss, trControl = trainControl(method = "cv", number = 5))

print(model_1)
print(model_2)
print(model_3)
```

Once you have your models fitted, use `print( )` to show the summary statistics for each model. Report the RMSE for each model, which is the square root of the MSE. Which of these models performs the best? Which performed the worst? Do these results agree with your expectations?

> The RMSE for model 1 is 7.736328, for model 2 is 8.186271, and for model 3 is 9.491742. Of these models, model 1 performed the best and model 3 performed the worst, both outcomes of which were expected.


Bonus: repeat the above step, using `trControl = trainControl(method="repeatedcv", number=5, repeats=3)` which repeats each CV analysis 3times and averages out each run to get a more stable estimate of the MSE. Compare the results with the unrepeated MSE estimates. How do they compare?


## Problem #2: More cars!  <small>4pts</small>

This `Auto` dataset, in the `Auto.csv` file contains measurements on over 300 cars. In this problem you will look at the effect of sample size and over-fitting. First load the data.

```{r}
Auto = read.csv("Auto.csv", stringsAsFactors = TRUE)
```

The dataset contains the response variable `mpg` and 7 predictor variables:

* `cylinders`  - the number of engine cylinders
* `displacement` - engine displacement
* `horsepower` 
* `weight`
* `acceleration`
* `year`
* `origin` - there are Asian, US and European cars; indicator variables have been added to the dataset for European and US cars.

The following function pulls the model formula out of the reg subsets object. It will be used later in the code. Be sure to run this chunk to put the function into the environment.
```{r}
# id: model id
# object: regsubsets object
# data: data used to fit regsubsets
# outcome: outcome variable
get_model_formula <- function(id, object, outcome){
  # get models data
  models <- summary(object)$which[id,-1]
  # Get outcome variable
  #form <- as.formula(object$call[[2]])
  #outcome <- all.vars(form)[1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+")
  # Build model formula
  as.formula(paste0(outcome, "~", predictors))
}  
```

This function, performs best subset model selection on a dataset with a specified response variable. It returns a list of models - the models with the lowest RSS for each model size from 1 to p. It will be used below. (Note: because of the origin variable a modification was made to only fit models up to size 7, not 8. This is because for a small sample size if there are not Asian, US and European cars the model fit will not work if all variables are included due to linear dependence among predictors)

```{r}
library(leaps)
getModels <- function (dataset, responseVar){
  models <- regsubsets(reformulate(".",responseVar), data = Auto.subset, nvmax = ncol(dataset)-2);
  modelList <- list("formula")
  nModels <- length(summary(models))-1
  for(i in 1:nModels){
    modelList[[i]] <- get_model_formula(i, models, responseVar)
  }
  return(modelList)  
}
```

Now we will run some code to answer the questions below. We will simulate having a small sample of cars to work with and fit the linear model. You will notice that in order to average over errors the entire simulation is performed `NMC=50` times. You may modify this if you wish. The primary line you will modify is where the sample size is set.

```{r, warning=FALSE}
sampleSize <- 200  #You should edit this number

NMC <- 50 #number of replications of this simulation
nFolds <- 5  #k=5 for 5-fold CV
nModels <- 7 # we will look at a maximum model size (# predictors) of 7.

errors <- data.frame('fold' = as.factor(rep(1:nFolds, nModels*NMC)),
                     'rep' = rep(1:NMC, each=nFolds*nModels),
                     'model' = rep(1:nModels, rep(nFolds, nModels)),
                     'mse' = rep(0, nModels*nFolds*NMC))
for(k in 1:NMC){
  Auto.subset <- Auto[sample(nrow(Auto), sampleSize),]
  modelList <- getModels(Auto.subset, "mpg")
  
  #Cross Validation
  folds <- split(sample(1:nrow(Auto.subset)), as.factor(1:nFolds))
  for(i in 1:nFolds){
    validation <- Auto.subset[folds[[i]],]
    training <- Auto.subset[-folds[[i]],]
    for(j in 1:nModels){
      fit <- lm(modelList[[j]], data=training)
      predictions <- predict(fit, newdata = validation)
      errors[errors$rep==k & errors$fold==i  & errors$model ==j, 'mse'] = mean((predictions-validation$mpg)^2)
    }
  }
}
avg <- aggregate(.~model, data=errors, FUN="mean")
plot(y=sqrt(avg$mse), x=avg$model, xlab="model", ylab="root mean square error", type="l", main="Comparison of Model Error")
```

a. If the sample size is 15, what is the size of the preferred model?

> The size of the preferred model is 3 predictors because the graph shows the lowest RMSE at 3 predictors.

b. If the sample size is larger, say 60, what is the size of the preferred model?

> The size of the preferred model is still 3 predictors, as the graph shows the lowest RMSE at that point.

c. Now consider if you have a sample of size 200. Does your preferred model change?

> The preferred model does change, for at a sample of size 200, the size of the preferred model increases to 5 predictors, because the RMSE is lowest at that point.

d. What is your general conclusion after looking at the effect of sample size on model size and model error?

> Generally, as sample size increases and more predictors are introduced, the model error decreases.


### Problem #3: Optimal K  <small>8pts; 2pts each</small>

Suppose the variable $Y=4 + 5X_1 + 8X_2 + \epsilon$ where $\epsilon \sim N(0, 2^2)$. Pretend this is the true model, but we don't know that - we are going to collect a random sample of size 40 and fit a linear model. We want to estimate the model error using $K-fold$ cross validation. In this problem we will figure out the optimal number of folds to get the best estimate of the model error $E(Y_{n+1}-\hat{Y}_{n+1})^2$.

We have to make a few assumptions to do this estimation. Let's suppose that $X_1 \sim N(3, 1^2)$ and $X_2 \sim N(1, .5^2)$. You can use the following function to simulate data:

```{r}
simulate.data <- function(n=40){
  X1 <- rnorm(n, 3, 1)
  X2 <- rnorm(n, 1, .5)
  eps <- rnorm(n, 0, 2)
  Y <- 4 + 5*X1 + 8*X2 + eps
  return(data.frame(Y,X1,X2))
}
```

### a. Estimate model error using Monte Carlo

Use Monte Carlo estimation to estimate the MSE of a linear model fit to a sample size of 40 using both predictors. On each MC repetition you should:

  i. generate a sample data set of size 40
  ii. fit a linear model using both X1 and X2 as predictors
  iii. simulate 1000 (or more) out of sample data points
  iv. calculate the square root of average squared error on the out of sample data points.

```{r}
NMC <- 1000
nUnseen <- 1000
Ehat <- 0 #an empty vector to store estimated 

for(i in 1:NMC){
  # generate a sample of size 40
  rep_v1 = simulate.data()
    
  #fit the linear model with X1 and X2 as predictors
  rep_model = lm(Y ~ 1 + X1 + X2, data = rep_v1)
  
  #simulate unseen data
  rep_v2 = simulate.data(nUnseen)
  
  #calculate the square root of the average squared error on the out of sample data points
  #store this in Ehat[i]
  Ehat[i] = sqrt(mean((abs(predict(rep_model, rep_v2) - rep_v2$Y)) ** 2))
}
(modelError <- mean(Ehat))
```

### b. Estimating MSE using CV
Now we imagine we don't know the true model error, but instead we want to estimate it with K-fold validation. The following function can be used to perform K-fold validation to estimate root mean squared error

```{r, warning=FALSE}
kfoldCV <- function(K, formula, dataset, responseVar){
  #idx is a shuffled vector of row numbers
  idx <- sample(1:nrow(dataset))
  #folds partitions the row indices
  folds <- suppressWarnings(split(idx, as.factor(1:K)))
  #an empty vector to hold estimated errors
  errors <- vector("numeric", K) 
  for(k in 1:K){
    #split the data into training and testing sets
    training <- dataset[-folds[[k]],]
    testing <- dataset[folds[[k]],]
    #go through each model and estimate MSE
    #fit the model to the training data
    fit <- lm(formula = formula, data=training)
    #calculate the sqrt of average squared error on the testing data
    errors[k] <- sqrt (mean((predict(fit, newdata=testing)-testing[,responseVar])^2))
  }
  return(mean(errors))
}
```

The following code runs an estimation simulation to help you see what happens to the estimate of model error as the number of folds increases. 
We will consider 2,3,4,5,6,8,10,15,20,30 and 40-fold CV.

```{r}
NMC <- 50
Ks <- c(2,3,4,5,6,8,10,15,20,30,40)
nK <- length(Ks)
formula <- reformulate(c("X1","X2"),"Y")

errors <- data.frame('replicate'=rep(1:NMC, each=nK),
                     'k' = rep(Ks, NMC),
                     'error' = rep(0, NMC*nK))
for(i in 1:NMC){
  myData <- simulate.data(40)
  for(k in Ks){
    errors[errors$replicate==i & errors$k==k, 'error'] <- kfoldCV(k, formula, myData, 'Y')
  }
}
averageErrors <- aggregate(error ~k, data=errors, FUN="mean")

plot(error ~ k, data=errors, col=rgb(0,0,0,.5))
lines(error ~ k, data=averageErrors, col="red", lwd=3)
abline(h=modelError)
```

From the code and plot generated answer the following questions:

i. What happens to the estimate of model error as the number of folds increases?

> As the number of folds increases, the estimate of model error decreases.

ii. Knowing the true model error, what number of folds seems to give the most unbiased estimate of model error?

> 5 folds seems to give the most unbiased estimate of model error.

iii. Besides the estimate being unbiased, what other consideration would you want to make when you consider the number of folds to choose?

> Another consideration that would be good to make is the size of the data set.

### c. The tradeoff
Finally look at this plot:
```{r}
vars = aggregate(error ~k, data = errors, FUN = "var")$error
bias2 = (averageErrors$error-2.07)^2
errors$errorsq = (errors$error-2.07)^2
mse = aggregate(errorsq ~ k, data = errors, FUN = "mean")$errorsq

plot(x = Ks, y = vars, ylim = c(0,max(mse)), type = "l", ylab = "")
lines(x = Ks, y = bias2, lty = 2, col = "blue")
lines(x = Ks, y = mse, lty = 3, lwd = 2, col = "red")
```

i. What does the solid black line represent? What pattern/trend do you see?

> The solid black line represents the relationship between K (# of folds) and variance, and I observe a trend in that variance flattens out/slightly decreases with more Ks.

ii. What does the dotted blue line represent? What pattern/trend do you observe?

> The dotted blue line represents the relationship between K (# of folds) and bias squared, and I observe a trend in that bias squared starts out at around 1 and increases as K does.

iii. What does the dotted red line represent? What pattern/trend do you observe?

> The dotted red line represents the relationship between K (# of folds) and mean squared error (MSE), and I observe a trend in that MSE starts out in same place as variance but then increases parallel to bias squared.

### d. Wrapping it up

Finally after all of this analysis, what number of folds would you conclude provides the best estimate of model error?

> The best estimate of model error is at the point where bias squared and variance intersect, which puts it at about 23-24 folds.



## Problem #4. Variable selection with `Carseats` <small>9pts (4 and 5)</small>

This question should be answered using the `Carseats` dataset from the `ISLR` package. If you do not have it, make sure to install it.

```{r}
library(ISLR)

Carseats = ISLR::Carseats

# you should read the help page by running ?Carseats
# we can also peek at the data frame before using it
str(Carseats)
head(Carseats)
```


### Part a) Visualizing/fitting

First, make some visualizations of the dataset to help set the stage for the rest of the analysis. Try to pick plots to show that are interesting informative.

```{r}
ggplot(Carseats) +
  geom_point(aes(x = Advertising, y = Sales), color = "purple") +
  ggtitle("Advertising v. Sales")

ggplot(Carseats) +
  geom_col(aes(x = Urban, y = Sales), color = "hotpink") +
  ggtitle("Urban v. Sales")

ggplot(Carseats) +
  geom_point(aes(x = Age, y = Sales), color = "skyblue") +
  ggtitle("Age v. Sales")
```

Using some variable selection method (stepwise, LASSO, ridge, or just manually comparing a preselected of models using their MSEs), choose a set of predictors to use to predict `Sales`. Try to find the best model that you can that explains the data well and doesn't have useless predictors. Explain the choices you made and show the final model.

```{r}
library(MASS)

lm1 = lm(Sales ~ 1, data=Carseats)
lm2 = lm(Sales ~ ., data=Carseats)

lm.forward = stepAIC(lm1, direction="forward", scope=list(upper=lm2,lower=lm1))
summary(lm.forward)
```

> My final model uses all available predictors except for population, urban, US, and education, since those were deemed to be the least significant predictors when running the forward stepwise variable selection method.

### Part b) Interpreting/assessing model

According to your chosen model, Which predictors appear to be the most important or significant in predicting sales? Provide an interpretation of each coefficient in your model. Be careful: some of the variables in the model are qualitative!

> According to my chosen model, the ShelveLoc, Price, CompPrice, Advertising, Age, and Income predictors appear to be the most important in predicting sales. The ShelveLoc predictor is split up into two variables, ShelveLocGood and ShelveLocMedium, and the coefficient for each of these respectively is 4.835675 and 1.951993, meaning that this shelf positioning will increase unit sales by about 4.84 and 1.95 thousand. The coefficient for the Price predictor is -0.095319, meaning that every unit increase in price will lessen sales by about 0.095 thousand. The coefficient for the CompPrice predictor is 0.092571, meaning that every unit increase in the comparative price will increase sales by about 0.093 thousand. The coefficient for the Advertising predictor is 0.115903, meaning that every unit increase in the advertising budget will increase sales by about 0.12 thousand. The coefficient for the Age predictor is -0.046128, meaning that every unit increase in age will decrease sales by about 0.05 thousand. The coefficient for the Income predictor is 0.015785, meaning that every unit increase in income will increase sales by about 0.02 thousand.

Estimate the out of sample MSE of your model and check any assumptions you made during your model fitting process. Discuss any potential model violations. How satisfied are you with your final model?

```{r}
car_model = lm(Sales ~ ShelveLoc + Price + CompPrice + Advertising + Age + Income, data = Carseats)
mean(abs(predict(car_model, Carseats) - Carseats$Sales) ** 2)
```

> The out of sample MSE is 1.018467. By using the stepwise forward method, which only checks if each predictor is important compared to predictors it has already tested (therefore not testing some combinations), we are assuming that the best model is not one of the ones we never tested. Overall, I am not too satisfied with this final model due to the uncertainty introduced by using the forward stepwise method.
